{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T07:47:18.366565Z",
     "start_time": "2024-05-08T07:47:18.347580Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T07:47:18.588415Z",
     "start_time": "2024-05-08T07:47:18.569851Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T07:47:22.668364Z",
     "start_time": "2024-05-08T07:47:18.791850Z"
    }
   },
   "outputs": [],
   "source": [
    "import json, pickle, array, datetime, pickle, os\n",
    "\n",
    "from statistics import mean, stdev\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from dataset.dataset_padchest import *\n",
    "\n",
    "from deap import base, creator, tools\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "import seaborn as sns\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "\n",
    "from XAI.evaluation import *\n",
    "from XAI.pareto_front import *\n",
    "from XAI.image_utils import *\n",
    "from XAI.genetic_algorithm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T07:47:22.781146Z",
     "start_time": "2024-05-08T07:47:22.728495Z"
    }
   },
   "outputs": [],
   "source": [
    "lat_dim = 500\n",
    "\n",
    "@tf.function()\n",
    "def sampling(args):\n",
    "    z_mean, z_log_sigma = args\n",
    "    epsilon = tf.random.normal(shape=(tf.shape(z_mean)[0], lat_dim), mean=0., stddev=0.1)\n",
    "    return z_mean + tf.math.exp(z_log_sigma) * epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data and model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T07:47:22.891785Z",
     "start_time": "2024-05-08T07:47:22.840770Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('config.json', 'r') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T07:47:25.361545Z",
     "start_time": "2024-05-08T07:47:22.949176Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "data = Dataset(config)\n",
    "\n",
    "with open(\"dataset.pkl\", 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T07:47:25.492224Z",
     "start_time": "2024-05-08T07:47:25.441762Z"
    }
   },
   "outputs": [],
   "source": [
    "LABELS = sorted(config[\"padchest\"][\"label_names\"], key=config[\"padchest\"][\"label_names\"].get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T07:47:25.596633Z",
     "start_time": "2024-05-08T07:47:25.548076Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_VER = 'v2_MOCVAE'\n",
    "MODEL_PATH = '/results_padchest_' + MODEL_VER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T07:47:33.852968Z",
     "start_time": "2024-05-08T07:47:25.652731Z"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder = load_model(MODEL_PATH + '/models/e_best_autoencoder.h5', custom_objects={'sampling': sampling})\n",
    "encoder = load_model(MODEL_PATH + '/models/e_best_encoder.h5', custom_objects={'sampling': sampling})\n",
    "decoder = load_model(MODEL_PATH + '/models/e_best_decoder.h5', custom_objects={'sampling': sampling})\n",
    "classifier = load_model(MODEL_PATH + '/models/e_best_classifier.h5', custom_objects={'sampling': sampling})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing all possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-08T07:51:52.607Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Label base: cardiomegaly - Label objective: aortic elongation\n",
      "Iteration 0 img_id: 7\n",
      "2024-05-08 07:51:53.346143\n"
     ]
    }
   ],
   "source": [
    "n_tests = 10\n",
    "out_base_path = 'XAI_evaluation_undersampling/'\n",
    "\n",
    "full_dict = dict()\n",
    "\n",
    "# Iterate over base labels\n",
    "for label_base in LABELS:\n",
    "    \n",
    "    os.makedirs(out_base_path + 'Input_Imgs/' + label_base, exist_ok=True)\n",
    "    \n",
    "    full_dict[label_base] = dict()\n",
    "    \n",
    "    for label_obj in LABELS:\n",
    "        full_dict[label_base][label_obj] = dict()\n",
    "        full_dict[label_base][label_obj]['means'] = dict()\n",
    "        full_dict[label_base][label_obj]['stds'] = dict()\n",
    "        for label_means in LABELS:\n",
    "            full_dict[label_base][label_obj]['means'][label_means] = []\n",
    "        for label_stds in LABELS:\n",
    "            full_dict[label_base][label_obj]['stds'][label_stds] = []\n",
    "        full_dict[label_base][label_obj]['histories'] = []\n",
    "        full_dict[label_base][label_obj]['population'] = []\n",
    "        full_dict[label_base][label_obj]['img_ids'] = []\n",
    "    \n",
    "    img_id = 0\n",
    "    \n",
    "    # Iterate n_test times to reduce variability\n",
    "    for i in range(n_tests):\n",
    "        enc=0\n",
    "        while enc==0:\n",
    "            # Find base case from a specific label\n",
    "            if LABELS[np.argmax(data.y_train[img_id])] == label_base:\n",
    "\n",
    "                # Find base case with confidence > 80%\n",
    "                input_img = data.X_train[img_id].reshape(1,224,224,1)\n",
    "                latent_code = encoder.predict(input_img)\n",
    "                latent_code = np.copy(latent_code[2])\n",
    "                classification = classifier.predict(latent_code)\n",
    "                conf = classification[0][LABELS.index(label_base)]\n",
    "\n",
    "                if conf > 0.8:\n",
    "                    enc=1\n",
    "                else:\n",
    "                    img_id+=1\n",
    "\n",
    "            else:\n",
    "                img_id+=1\n",
    "\n",
    "        # Save Image\n",
    "        img = data.X_train[img_id].reshape(224,224)\n",
    "        matplotlib.image.imsave(out_base_path + 'Input_Imgs/' + label_base + '/'\n",
    "                    + label_base + '_' + str(i) + '.png', img, cmap='gray')\n",
    "        \n",
    "        # Iterate over objective labels\n",
    "        for label_obj in LABELS:\n",
    "            if label_base != label_obj:\n",
    "                print(\"\\n\\nLabel base:\", label_base, '- Label objective:', label_obj)\n",
    "                print(\"Iteration\", i, \"img_id:\", img_id)\n",
    "                print(datetime.now())\n",
    "\n",
    "                input_img = data.X_train[img_id].reshape(1,224,224,1)\n",
    "                latent_code = encoder.predict(input_img)\n",
    "                # Use sampled z as latent space\n",
    "                latent_code = latent_code[2]\n",
    "\n",
    "                creator, toolbox = deap_configuration(latent_code)\n",
    "\n",
    "                # Genetic optimization\n",
    "                pop = toolbox.population(n=80)\n",
    "\n",
    "                final_set, history = spea2(base_ind=latent_code, classifier=classifier,\n",
    "                                           label_obj=label_obj, lab_list=LABELS,\n",
    "                                           pop=pop, toolbox=toolbox, num_gens=250,\n",
    "                                           sel_factor_pop=80, sel_factor_arch=40,\n",
    "                                           mut_prob=MUTPB, mutrevpb=MUTREVPB, indrevpb=INDREVPB)\n",
    "                \n",
    "                # Purge final population keeping only individuals that missclasify\n",
    "                class_ch = get_class_changes(latent_code, classifier, final_set, LABELS)\n",
    "                final_set_purged = [final_set[i] for i in range(len(class_ch)) if class_ch[i] == 1]\n",
    "                \n",
    "                if len(final_set_purged)>0:\n",
    "                    # Calculate change in classification\n",
    "                    changes_dict = get_conf_changes_dict(data, encoder, classifier, label_obj, LABELS, final_set_purged)\n",
    "\n",
    "                    means = {key: np.mean(value['confidence_ch']) for key, value in changes_dict.items()}\n",
    "                    stds = {key: np.std(value['confidence_ch']) for key, value in changes_dict.items()}\n",
    "\n",
    "                    for label in means.keys():\n",
    "                        full_dict[label_base][label_obj]['means'][label].append(means[label])\n",
    "\n",
    "                    for label in stds.keys():\n",
    "                        full_dict[label_base][label_obj]['stds'][label].append(stds[label])\n",
    "                    \n",
    "                full_dict[label_base][label_obj]['histories'].append(history)\n",
    "                full_dict[label_base][label_obj]['population'].append(final_set)\n",
    "                full_dict[label_base][label_obj]['img_ids'].append(img_id)\n",
    "                \n",
    "                class_chs = get_class_changes_obj(latent_code, classifier, final_set, label_obj, LABELS)\n",
    "                fitnesses = evaluate_pop(final_set, latent_code, classifier, label_obj, LABELS)\n",
    "                fitnesses_purged = [list(fitnesses[j][1:]) for j in range(len(class_chs)) if class_chs[j] == 1]\n",
    "                final_set_purged = [final_set[j] for j in range(len(class_chs)) if class_chs[j] == 1]\n",
    "                \n",
    "                save_path = out_base_path + 'MOCVAE/min_num/' + label_base + '-' + label_obj + '_' + str(i)\n",
    "                idx_min_num = min(range(len(fitnesses_purged)), key=lambda i: fitnesses_purged[i][1])\n",
    "                min_num_ind = final_set_purged[idx_min_num]\n",
    "                plot_ind_changes(data, encoder, decoder, classifier,\n",
    "                                 img_id, min_num_ind, LABELS, save_path)\n",
    "                \n",
    "                save_path = out_base_path + 'MOCVAE/min_mag/' + label_base + '-' + label_obj + '_' + str(i)\n",
    "                idx_min_mag = min(range(len(fitnesses_purged)), key=lambda i: fitnesses_purged[i][0])\n",
    "                min_mag_ind = final_set_purged[idx_min_mag]\n",
    "                plot_ind_changes(data, encoder, decoder, classifier,\n",
    "                                 img_id, min_mag_ind, LABELS, save_path)\n",
    "                \n",
    "        img_id+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-05-08T07:51:52.793Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"full_dict_undersampling_\" + MODEL_VER + \".pkl\", 'wb') as f:\n",
    "        pickle.dump(full_dict, f)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOWnpEtbtBYdwzbkV75yS8R",
   "collapsed_sections": [],
   "name": "Autoencoder CBIR.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "512px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "350.844px",
    "left": "2191px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
